(window.webpackJsonp=window.webpackJsonp||[]).push([["d0a3"],{"+a94":function(e){e.exports={bodyContent:"**Organizer:** UIST 2017 (Web and Social Media Chair)\n\n**Review:** CHI 2016--2019, UIST 2017--2019, SCF 2019\n\n**Student Volunteer:** UIST 2016, CHI 2017",bodyHtml:"<p><strong>Organizer:</strong> UIST 2017 (Web and Social Media Chair)</p>\n<p><strong>Review:</strong> CHI 2016--2019, UIST 2017--2019, SCF 2019</p>\n<p><strong>Student Volunteer:</strong> UIST 2016, CHI 2017</p>\n",title:"<strong>Organizer:",dir:"content/output",base:"activities.json",ext:".json",sourceBase:"activities.md",sourceExt:".md"}},"42TL":function(e,t,a){"use strict";a.r(t);var i=a("0iUn"),n=a("sLSF"),o=a("MI3g"),r=a("a7VT"),s=a("Tit0"),c=a("q1tI"),l=a.n(c),d=(a("IujW"),a("MyTI")),p=function(e){function t(){return Object(i.default)(this,t),Object(o.default)(this,Object(r.default)(t).apply(this,arguments))}return Object(s.default)(t,e),Object(n.default)(t,[{key:"render",value:function(){return l.a.createElement("div",{id:"updates",className:"ui relaxed divided list"},l.a.createElement("h3",null,"Research Experience"),d.map(function(e){return l.a.createElement("div",{className:"item",style:{padding:"20px 0"}},l.a.createElement("div",{className:"ui mini image"},l.a.createElement("img",{src:"/static/images/".concat(e.logo)})),l.a.createElement("div",{className:"middle aligned content"},l.a.createElement("div",{className:"header"},e.institute.name,l.a.createElement("br",null)),l.a.createElement("div",{className:"description"},l.a.createElement("a",{href:e.lab.url},l.a.createElement("b",null,e.lab.name)))),l.a.createElement("div",{className:"content"},l.a.createElement("div",{className:"ui list"},e.advisors.map(function(e){return l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:e.url},e.name))}))),l.a.createElement("div",{className:"extra",style:{marginLeft:"0px",marginTop:"5px"}},l.a.createElement("div",{className:"ui label"},e.period)))}))}}]),t}(l.a.Component);t.default=p},"6T/A":function(e){e.exports={}},"9Gwv":function(e){e.exports=[{date:"2019-03-26",image:"dis-2019.png",text:"One [**DIS 2019**](https://dis2019.com/) full-paper is accepted (25%)."},{date:"2018-08-06",image:"uist-2018.png",text:"One [**UIST 2018**](https://uist.acm.org/uist2018/) full-paper is accepted (21%)."},{date:"2018-08-03",image:"pg-2018.png",text:"One [**Pacific Graphics 2018**](http://sweb.cityu.edu.hk/pg2018/) short-paper is accepted (26%)."},{date:"2017-12-11",image:"chi-2018.png",text:"Two [**CHI 2018**](https://chi2018.acm.org/) full-papers are accepted (25%)."},{date:"2017-06-27",image:"vlhcc-2017.png",text:"One [**VL/HCC 2017**](https://sites.google.com/site/vlhcc2017/) full-paper are accepted (29%)."},{date:"2017-06-21",image:"assets-2017.png",text:"One [**ASSETS 2017**](https://assets17.sigaccess.org/) full-paper are accepted (26%)."},{date:"2017-02-11",image:"chi-2017.png",text:"One [**CHI 2017**](https://chi2017.acm.org/) LBW paper is accepted (38%)."},{date:"2016-12-16",image:"",text:"One [**L@S 2017**](http://learningatscale.acm.org/las2017/) full paper is accepted (22%)."},{date:"2016-12-12",image:"icse-2017.png",text:"One [**ICSE 2017**](http://icse2017.gatech.edu/) full-paper is accepted (19%)."},{date:"2016-01-15",image:"chi-2016.png",text:"One [**CHI 2016**](https://chi2016.acm.org/wp/) full-paper is accepted (23%)."},{date:"2015-10-01",image:"uist-2016.jpg",text:"I will serve as a web and social media chair for [**UIST 2016**](http://uist.acm.org/uist2016/)"}]},CTYI:function(e){e.exports={id:"flux-marker",name:"FluxMarker",description:"Enhancing Tactile Graphics with Dynamic Tactile Markers for Blind People",title:"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers",authors:"Ryo Suzuki, Abigale Stangl, Mark D. Gross, Tom Yeh",image:"fluxmarker.png",conference:{name:"ASSETS 2017",url:"https://assets17.sigaccess.org/"},pdf:"assets-2017-fluxmarker.pdf",video:"https://www.youtube.com/watch?v=VbwIZ9V6i_g",slide:"assets-2017-fluxmarker-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3132548",arxiv:"https://arxiv.org/abs/1708.03783",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"flux-marker.json",ext:".json",sourceBase:"flux-marker.md",sourceExt:".md"}},"Dx8+":function(e,t,a){"use strict";a.r(t);var i=a("0iUn"),n=a("sLSF"),o=a("MI3g"),r=a("a7VT"),s=a("Tit0"),c=a("q1tI"),l=a.n(c),d=a("IujW"),p=a.n(d),m=a("9Gwv"),h=function(e){function t(){return Object(i.default)(this,t),Object(o.default)(this,Object(r.default)(t).apply(this,arguments))}return Object(s.default)(t,e),Object(n.default)(t,[{key:"render",value:function(){return l.a.createElement("div",{id:"updates",className:"ui relaxed divided list"},l.a.createElement("h3",null,"Recent Updates"),m.map(function(e){return l.a.createElement("div",{className:"item"},l.a.createElement("div",{className:"header"},e.date),l.a.createElement("div",{className:"content"},l.a.createElement(p.a,{source:e.text})))}))}}]),t}(l.a.Component);t.default=h},GbvX:function(e){e.exports={id:"dynablock",name:"Dynablock",description:"Dynamic 3D Printing for Instant and Reconstructable Shape Formation",title:"Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation",authors:"Ryo Suzuki, Junichi Yamaoka, Daniel Leithinger, Tom Yeh Mark D. Gross, Yoshihiro Kawahara, Yasuaki Kakehi",image:"dynablock.jpg",conference:{name:"UIST 2018",url:"http://uist.acm.org/uist2018"},pdf:"uist-2018-dynablock.pdf",video:"https://www.youtube.com/watch?v=7nPlr3O9xu8","short-video":"https://www.youtube.com/watch?v=92eGI-gYYc4",slide:"uist-2018-dynablock-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3242659",talk:"https://www.youtube.com/watch?v=R3FRUtOIiCQ",bodyContent:'# Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation\n\n**Ryo Suzuki**, Junichi Yamaoka, Daniel Leithinger, Tom Yeh Mark D. Gross, Yoshihiro Kawahara, Yasuaki Kakehi\n\n[**The ACM Symposium on User Interface Software and Technology (UIST 2018)**](http://uist.acm.org/uist2018/)\n\nLinks:\n[**[PDF](http://ryosuzuki.org/publications/uist-2018-dynablock.pdf)**]\n[**[ACM DL](https://dl.acm.org/citation.cfm?id=3242659)**]\n[**[Video](https://www.youtube.com/watch?v=7nPlr3O9xu8)**]\n[**[Slide](http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf)**]\n[**[Talk](https://www.youtube.com/watch?v=R3FRUtOIiCQ)**]\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/top.mp4" type="video/mp4"></source>\n</video>\n\n\n# Abstract\n\nThis paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.\n\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-2.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-3.jpg" /></a>\n  </div>\n</div>\n\n# Dynamic 3D Printing\n\nWhat if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Todayâ€™s 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.\n\nThis paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n\nWe define Dynamic 3D Printing as a class of systems that have the following properties:\n\n- Immediate: The system can form a physical shape in sec- onds.\n\n- Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.\n\n- Arbitrary Shapes: It can create arbitrary three dimensional shapes.\n\n- Graspable: The output shapes and structure are graspable and solid.\n\n# Parallel Assembler\n\n<div class="figures ui stackable one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-3-1.png" /></a>\n  </div>\n</div>\n\nDynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/mechanism.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-4-1.png" /></a>\n  </div>\n  <p class="column">\n    This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n  </p>\n</div>\n\n\n# Implementation\n\nThe assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet (Ï† 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.\n\n\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-3.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-3.jpg" /></a>\n  </div>\n</div>\n\n\n# Future Vision\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/claytronics.mp4" type="video/mp4"></source>\n</video>\n\nWith these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the productâ€™s design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.\n\n<div class="figures ui stackable one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-8-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-8-1.png" /></a>\n  </div>\n</div>\n\nDynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.\n\n\n## Video Preview\n\n<div class="video-container">\n  <iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/7nPlr3O9xu8?autoplay=1&mute=1&rel=0&loop=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n</div>',bodyHtml:'<h1>Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation</h1>\n<p><strong>Ryo Suzuki</strong>, Junichi Yamaoka, Daniel Leithinger, Tom Yeh Mark D. Gross, Yoshihiro Kawahara, Yasuaki Kakehi</p>\n<p><a href="http://uist.acm.org/uist2018/"><strong>The ACM Symposium on User Interface Software and Technology (UIST 2018)</strong></a></p>\n<p>Links:\n[<strong><a href="http://ryosuzuki.org/publications/uist-2018-dynablock.pdf">PDF</a></strong>]\n[<strong><a href="https://dl.acm.org/citation.cfm?id=3242659">ACM DL</a></strong>]\n[<strong><a href="https://www.youtube.com/watch?v=7nPlr3O9xu8">Video</a></strong>]\n[<strong><a href="http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf">Slide</a></strong>]\n[<strong><a href="https://www.youtube.com/watch?v=R3FRUtOIiCQ">Talk</a></strong>]</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/top.mp4" type="video/mp4"></source>\n</video>\n<h1>Abstract</h1>\n<p>This paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.</p>\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-2.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-3.jpg" /></a>\n  </div>\n</div>\n<h1>Dynamic 3D Printing</h1>\n<p>What if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Todayâ€™s 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.</p>\n<p>This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.</p>\n<p>We define Dynamic 3D Printing as a class of systems that have the following properties:</p>\n<ul>\n<li>\n<p>Immediate: The system can form a physical shape in sec- onds.</p>\n</li>\n<li>\n<p>Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.</p>\n</li>\n<li>\n<p>Arbitrary Shapes: It can create arbitrary three dimensional shapes.</p>\n</li>\n<li>\n<p>Graspable: The output shapes and structure are graspable and solid.</p>\n</li>\n</ul>\n<h1>Parallel Assembler</h1>\n<div class="figures ui stackable one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-3-1.png" /></a>\n  </div>\n</div>\n<p>Dynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/mechanism.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-4-1.png" /></a>\n  </div>\n  <p class="column">\n    This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n  </p>\n</div>\n<h1>Implementation</h1>\n<p>The assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet (Ï† 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.</p>\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-3.png" /></a>\n  </div>\n</div>\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-3.jpg" /></a>\n  </div>\n</div>\n<h1>Future Vision</h1>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/claytronics.mp4" type="video/mp4"></source>\n</video>\n<p>With these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the productâ€™s design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.</p>\n<div class="figures ui stackable one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-8-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-8-1.png" /></a>\n  </div>\n</div>\n<p>Dynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.</p>\n<h2>Video Preview</h2>\n<div class="video-container">\n  <iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/7nPlr3O9xu8?autoplay=1&mute=1&rel=0&loop=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n</div>',dir:"content/output/projects",base:"dynablock.json",ext:".json",sourceBase:"dynablock.md",sourceExt:".md"}},Jg5j:function(e){e.exports={id:"trace-diff",name:"TraceDiff",description:"Debugging Unexpected Code Behavior Using Trace Divergences",title:"TraceDiff: Debugging Unexpected Code Behavior Using Trace Divergences",authors:"Ryo Suzuki, Gustavo Soares, Andrew Head, Elena Glassman, Ruan Reis, Melina Mongiovi, Loris Dâ€™Antoni, Bjoern Hartmann",image:"tracediff.png",conference:{name:"VL/HCC 2017",url:"https://sites.google.com/site/vlhcc"},pdf:"vlhcc-2017-tracediff.pdf",slide:"vlhcc-2017-tracediff-slide.pdf",github:"https://github.com/ryosuzuki/trace-diff",demo:"https://ryosuzuki.github.io/trace-diff/",ieee:"http://ieeexplore.ieee.org/document/8103457/",arxiv:"https://arxiv.org/abs/1708.03786a",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"trace-diff.json",ext:".json",sourceBase:"trace-diff.md",sourceExt:".md"}},LrW7:function(e){e.exports={}},MyTI:function(e){e.exports=[{period:"May, 2019 --- Current",role:"Intern",logo:"adobe.png",institute:{name:"Adobe Research",url:"https://colorado.edu"},lab:{name:"Creative Intelligence Lab",url:"http://hcc.colorado.edu/"},advisors:[{name:"Rubaiat Habib",url:"https://rubaiathabib.me/"},{name:"Li-Yi Wei",url:"https://www.liyiwei.org/"},{name:"Stephen Diverdi",url:"http://www.stephendiverdi.com/"},{name:"Danny Kaufman",url:"http://dannykaufman.io/"}]},{period:"August, 2015 --- Current",role:"RA",logo:"cu-boulder.png",institute:{name:"CU Boulder",url:"https://colorado.edu"},lab:{name:"THING Lab",url:"https://www.colorado.edu/atlas/thing-lab"},advisors:[{name:"Daniel Lightinger",url:"http://leithinger.com/"},{name:"Mark D. Gross",url:"http://mdgross.net/"},{name:"Tom Yeh",url:"http://tomyeh.info/"}]},{period:"December, 2017 --- October, 2018",role:"Intern",logo:"ut.png",institute:{name:"University of Tokyo",url:"https://colorado.edu"},lab:{name:"ERATO UIN",url:"http://www.jst.go.jp/erato/kawahara"},advisors:[{name:"Yasuaki Kakehi",url:"http://www.xlab.sfc.keio.ac.jp/"},{name:"Yoshihiro Kawahara",url:"http://www.akg.t.u-tokyo.ac.jp/"},{name:"Ryuma Niiyama",url:"https://scholar.google.co.jp/citations?user=0NMf5sgAAAAJ&hl=en"}]},{period:"May, 2016 --- August, 2016",role:"Intern",logo:"uc-berkeley.png",institute:{name:"UC Berkeley",url:null},lab:{name:"BiD Lab",url:"http://bid.berkeley.edu/"},advisors:[{name:"Bjoern Hartmann",url:"http://people.eecs.berkeley.edu/~bjoern/"}]},{period:"May, 2015 --- August, 2015",role:"Intern",logo:"stanford.png",institute:{name:"Stanford University",url:"https://stanford.edu"},lab:{name:"HCI Group",url:"http://hci.stanford.edu/"},advisors:[{name:"Michael Bernstein",url:"http://hci.stanford.edu/msb/"}]},{period:"October, 2014 --- May, 2015",role:"RA",logo:"ut.png",institute:{name:"University of Tokyo",url:null},lab:{name:"IIS Lab",url:"http://iis-lab.org/"},advisors:[{name:"Koji Yatani",url:"http://iis-lab.org/member/koji-yatani/"}]},{period:"December, 2014 --- March, 2015",role:"Intern",logo:"aist.png",institute:{name:"AIST",url:null},lab:{name:"Media Interaction",url:"https://staff.aist.go.jp/m.goto/MIG/index-j.html"},advisors:[{name:"Jun Kato",url:"http://junkato.jp/"}]}]},PSd4:function(e){e.exports={id:"mixed-initiative",name:"Mixed-Initiative Code Feedback",description:"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis",title:"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis",authors:"Andrew Head*, Elena Glassman*, Gustavo Soares*, Ryo Suzuki, Lucas Figueredo, Loris Dâ€™Antoni, Bjoern Hartmann (* equally contributed)",image:"mixed-initiative.png",conference:{name:"L@S 2017",url:"http://learningatscale.acm.org/las2017"},pdf:"las-2017-mixed.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3051467",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"mixed-initiative.json",ext:".json",sourceBase:"mixed-initiative.md",sourceExt:".md"}},RHEb:function(e,t,a){"use strict";a.r(t);for(var i=a("0iUn"),n=a("sLSF"),o=a("MI3g"),r=a("a7VT"),s=a("Tit0"),c=a("q1tI"),l=a.n(c),d=(a("IujW"),a("6T/A"),[]),p=0,m=["morphio","dynablock","tabby","reactile","pep","flux-marker","trace-diff","mixed-initiative","refazer","atelier"];p<m.length;p++){var h=m[p],u=a("o0EK")("./".concat(h,".json"));d.push(u)}var g=function(e){function t(){return Object(i.default)(this,t),Object(o.default)(this,Object(r.default)(t).apply(this,arguments))}return Object(s.default)(t,e),Object(n.default)(t,[{key:"componentDidMount",value:function(){}},{key:"render",value:function(){return l.a.createElement("div",{id:"projects"},l.a.createElement("h1",null,"Full Papers"),d.map(function(e){return l.a.createElement("div",{className:"project ui vertical segment grid","data-id":e.id},l.a.createElement("div",{className:"four wide column"},l.a.createElement("a",{href:"/".concat(e.id)},l.a.createElement("img",{className:"ui rounded images",src:"/static/images/".concat(e.image)}))),l.a.createElement("div",{className:"twelve wide column"},l.a.createElement("h1",{className:"ui header",style:{marginBottom:"10px"}},l.a.createElement("a",{href:"/".concat(e.id)},l.a.createElement("span",null,e.name)),l.a.createElement("span",{className:"ui big label"},e.conference.name)),l.a.createElement("h2",{style:{margin:"5px 0"}},e.description),l.a.createElement("p",null,e.authors),l.a.createElement("a",{href:"/publications/"+e.pdf,target:"blank",style:{marginRight:"5px",display:e.pdf?"inline":"none"}},"[PDF]"),l.a.createElement("a",{href:e.video,target:"blank",style:{marginRight:"5px",display:e.video?"inline":"none"}},"[Video]"),l.a.createElement("a",{href:e["short-video"],target:"blank",style:{marginRight:"5px",display:e["short-video"]?"inline":"none"}},"[Short Video]"),l.a.createElement("a",{href:"/publications/"+e.slide,target:"blank",style:{marginRight:"5px",display:e.slide?"inline":"none"}},"[Slide]"),l.a.createElement("a",{href:e.github,target:"blank",style:{marginRight:"5px",display:e.github?"inline":"none"}},"[GitHub]"),l.a.createElement("a",{href:e["acm-dl"],target:"blank",style:{marginRight:"5px",display:e["acm-dl"]?"inline":"none"}},"[ACM DL]"),l.a.createElement("a",{href:e.ieee,target:"blank",style:{marginRight:"5px",display:e.ieee?"inline":"none"}},"[IEEE]"),l.a.createElement("a",{href:e.talk,target:"blank",style:{marginRight:"5px",display:e.talk?"inline":"none"}},"[Talk]")),l.a.createElement("div",{id:e.id,className:"ui modal"},l.a.createElement("div",{className:"content"},l.a.createElement("h1",null,e.title),l.a.createElement("div",{className:"ui horizontal divider"}),l.a.createElement("div",{className:"authors"},l.a.createElement("h3",null,"Authors"),l.a.createElement("div",{className:"authors ui very relaxed horizontal divided list"},l.a.createElement("div",{className:"item"},e.authors))),l.a.createElement("div",{className:"ui horizontal divider"}),l.a.createElement("div",{className:"video"},l.a.createElement("h3",null,"Video Preview"),l.a.createElement("div",{className:"video-container"},l.a.createElement("iframe",{id:"video",width:"560",height:"300",src:"https://www.youtube.com/embed/-JcezIL3UKQ",frameBorder:"0",allowFullScreen:!0}))),l.a.createElement("div",{className:"ui horizontal divider"}),l.a.createElement("div",{className:"abstract"},l.a.createElement("h3",null,"Abstract"),e.abstract)),l.a.createElement("div",{className:"actions"},l.a.createElement("div",{className:"ui approve button"},"Approve"),l.a.createElement("div",{className:"ui button"},"Neutral"),l.a.createElement("div",{className:"ui cancel button"},"Cancel"))))}))}}]),t}(l.a.Component);t.default=g},RNiq:function(e,t,a){"use strict";a.r(t);var i=a("0iUn"),n=a("sLSF"),o=a("MI3g"),r=a("a7VT"),s=a("Tit0"),c=a("q1tI"),l=a.n(c),d=a("IujW"),p=a.n(d),m=(a("TIwn"),a("MyTI"),a("yC4j")),h=a("+a94"),u=(a("LrW7"),a("qg4i")),g=a("W+IF"),f=a("RHEb"),b=a("42TL"),v=a("Dx8+"),y=function(e){function t(){return Object(i.default)(this,t),Object(o.default)(this,Object(r.default)(t).apply(this,arguments))}return Object(s.default)(t,e),Object(n.default)(t,[{key:"componentDidMount",value:function(){}},{key:"render",value:function(){return l.a.createElement("div",null,l.a.createElement("div",{className:"ui stackable grid"},l.a.createElement("div",{className:"one wide column"}),l.a.createElement("div",{className:"eleven wide column centered"},l.a.createElement(g.default,null),l.a.createElement("section",{id:"container"},l.a.createElement(f.default,null),l.a.createElement("div",{class:"ui divider"}),l.a.createElement("div",{id:"posters"},l.a.createElement("h1",null,"Posters and Demos"),l.a.createElement("div",{className:"ui vertical segment"},l.a.createElement("div",{className:"ui bulleted list"},u.map(function(e){return l.a.createElement("div",{className:"item",target:"_blank",style:{lineHeight:"1.8rem"}},l.a.createElement("b",null,"[",e.series,"]"),l.a.createElement("br",null),l.a.createElement("a",{href:"/publications/"+e.pdf},l.a.createElement("b",null,e.title)),l.a.createElement("br",null),e.author,", ",l.a.createElement("i",null,e.booktitle," (",e.series,")"),". ",e.publisher,", ",e.address,", ",e.pages,".",l.a.createElement("br",null),l.a.createElement("a",{href:"/publications/"+e.pdf,target:"_blank",style:{marginRight:"5px",display:e.pdf?"inline":"none"}},"[PDF]"),l.a.createElement("a",{href:"/publications/"+e.poster,target:"_blank",style:{marginRight:"5px",display:e.poster?"inline":"none"}},"[Poster]"),l.a.createElement("a",{href:"/publications/"+e.slide,target:"_blank",style:{marginRight:"5px",display:e.slide?"inline":"none"}},"[Slide]"),l.a.createElement("a",{href:e.url,target:"_blank",style:{marginRight:"5px",display:e.url?"inline":"none"}},"[DOI]"))})))),l.a.createElement("div",{class:"ui divider"}),l.a.createElement("div",{id:"activities"},l.a.createElement("h1",null,"Professional Activities"),l.a.createElement("div",{className:"ui vertical segment"},l.a.createElement(p.a,{source:h.bodyContent}))),l.a.createElement("div",{class:"ui divider"}),l.a.createElement("div",{id:"fellowship"},l.a.createElement("h1",null,"Scholarship and Fellowship"),l.a.createElement("div",{className:"ui vertical segment"},l.a.createElement(p.a,{source:m.bodyContent}))))),l.a.createElement("div",{id:"side",className:"three wide column centered",style:{marginTop:"50px"}},l.a.createElement(b.default,null),l.a.createElement(v.default,null),l.a.createElement("br",null),l.a.createElement("a",{class:"twitter-timeline",height:"1500px",href:"https://twitter.com/ryosuzk?ref_src=twsrc%5Etfw"},"Tweets by @ryosuzk")," ",l.a.createElement("script",{async:!0,src:"https://platform.twitter.com/widgets.js",charset:"utf-8"})),l.a.createElement("div",{className:"one wide column"})),l.a.createElement("div",{className:"ui stackable grid"},l.a.createElement("div",{className:"sixteen wide column centered"},l.a.createElement("p",{style:{textAlign:"center"}}))))}}]),t}(l.a.Component);t.default=y},"W+IF":function(e,t,a){"use strict";a.r(t);var i=a("0iUn"),n=a("sLSF"),o=a("MI3g"),r=a("a7VT"),s=a("Tit0"),c=a("q1tI"),l=a.n(c),d=function(e){function t(){return Object(i.default)(this,t),Object(o.default)(this,Object(r.default)(t).apply(this,arguments))}return Object(s.default)(t,e),Object(n.default)(t,[{key:"render",value:function(){return l.a.createElement("header",{className:"ui stackable grid"},l.a.createElement("div",{className:"ui sixteen wide column"},l.a.createElement("h1",{className:"ui huge header"},l.a.createElement("img",{style:{maxWidth:"62px",marginRight:"15px"},src:"/static/images/profile.png",className:"ui circular image"}),l.a.createElement("div",{className:"content"},"Ryo Suzuki",l.a.createElement("div",{className:"sub header",style:{fontSize:"1.5rem"}},"University of Colorado Boulder"))),l.a.createElement("video",{id:"top-video",poster:"/static/images/top.png",preload:"metadata",autoPlay:!0,loop:!0,muted:!0,playsInline:!0,"webkit-playsinline":""},l.a.createElement("source",{src:"/static/videos/top.webm",type:"video/webm"}),l.a.createElement("source",{src:"/static/videos/top.mp4",type:"video/mp4"})),l.a.createElement("div",{id:"profile",style:{fontSize:"1.3rem"}},l.a.createElement("p",null,"I am a Ph.D. student at the ",l.a.createElement("a",{href:"https://www.colorado.edu/cs/",target:"_blank"},l.a.createElement("b",null,"University of Colorado Boulder")),", Department of Computer Science, advised by ",l.a.createElement("a",{href:"http://leithinger.com/",target:"_blank"},l.a.createElement("b",null,"Daniel Leithinger"))," and ",l.a.createElement("a",{href:"http://mdgross.net/",target:"_blank"},l.a.createElement("b",null,"Mark D. Gross"))," in ",l.a.createElement("a",{href:"https://www.colorado.edu/atlas/thing-lab",target:"_blank"},l.a.createElement("b",null,"THING Lab"))," and ",l.a.createElement("a",{href:"http://hcc.colorado.edu/",target:"_blank"},l.a.createElement("b",null,"Human-Computer Interaction Group"))),l.a.createElement("p",null,"I make ",l.a.createElement("b",null,"a dynamic medium")," for ",l.a.createElement("b",null,"human-computer interaction"),". More specifically, I explore a novel computational medium that can potentially augment and transform our ways of thinking, designing, understanding, creating, communicating, and exploring ideas. I believe such a computational medium will not be only limited on a computer screen, but will become ",l.a.createElement("b",null,"a whole environment"),", including space and physical objects, that can dynamically change and leverage our entire bodies to explore ideas, just like what we do in a science museum.",l.a.createElement("br",null))),l.a.createElement("div",{className:"ui horizontal list",style:{marginTop:"10px"}},l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://scholar.google.com/citations?user=klWjaQIAAAAJ",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fas fa-graduation-cap fa-fw"}),"Google Scholar")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"/cv.pdf",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"far fa-file fa-fw"}),"Resume/CV")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"mailto:ryo.suzuki@colorado.edu",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"far fa-envelope fa-fw"}),"ryo.suzuki@colorado.edu")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://www.facebook.com/ryosuzk",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-facebook-square fa-fw"}),"ryosuzk")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://twitter.com/ryosuzk",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-twitter fa-fw"}),"ryosuzk")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://github.com/ryosuzuki",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-github-alt fa-fw"}),"ryosuzuki")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://www.linkedin.com/in/ryosuzuki/",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-linkedin-in fa-fw"}),"ryosuzuki")))),l.a.createElement("div",{className:"one wide column"}))}}]),t}(l.a.Component);t.default=d},"W/HP":function(e){e.exports={id:"pep",name:"PEP",description:"3D Printed Electronic Papercrafts - An Integrated Approach for 3D Sculpting Paper-based Electronic Devices",title:"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-based Electronic Devices",authors:"Hyunjoo Oh, Tung D. Ta, Ryo Suzuki, Mark D. Gross, Yoshihiro Kawahara, Lining Yao",image:"pep.png",conference:{name:"CHI 2018",url:"https://chi2018.acm.org/"},pdf:"chi-2018-pep.pdf",video:"https://vimeo.com/252080903","short-video":"https://www.youtube.com/watch?v=DTd863suDN0","acm-dl":"https://dl.acm.org/citation.cfm?id=3174015",bodyContent:"## Abstract\n\nhogehoge\nfugafuga",bodyHtml:"<h2>Abstract</h2>\n<p>hogehoge\nfugafuga</p>\n",dir:"content/output/projects",base:"pep.json",ext:".json",sourceBase:"pep.md",sourceExt:".md"}},"X0/d":function(e){e.exports={id:"morphio",name:"MorphIO",description:"Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",title:"MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",authors:"Ryosuke Nakayama*, Ryo Suzuki*, Satoshi Nakamaru, Ryuma Niiyama, Yoshihiro Kawahara, Yasuaki Kakehi (* equally contributed)",image:"morphio.png",conference:{name:"DIS 2019",url:"https://dis2019.com/"},pdf:"dis-2019-morphio.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3322337",bodyContent:'# MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction\n\nRyosuke Nakayama\\*, **Ryo Suzuki\\* **, Satoshi Nakamaru, Ryuma Niiyama, Yoshihiro Kawahara, Yasuaki Kakehi\n<br/>\n(the first two authors equally contributed)\n\n[**The ACM conference on Designing Interactive Systems (DIS 2019)**](https://dis2019.com/)\n--- **Best Paper Award**\n\nLinks:\n[**[PDF](http://ryosuzuki.org/publications/dis-2019-morphio.pdf)**]\n[**[ACM DL](https://dl.acm.org/citation.cfm?id=3322337)**]\n[**[Video](https://www.youtube.com/watch?v=7nPlr3O9xu8)**]\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/top.mp4" type="video/mp4"></source>\n</video>\n\n\n# Abstract\n\nWe introduce MorphIO, entirely soft sensing and actuation modules for programming by demonstration of soft robots and shape-changing interfaces. MorphIOâ€™s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows both input and output of three-dimensional defor- mation of a soft material. Leveraging this capability, MorphIO enables a user to record and later playback physical motion of programmable shape-changing materials. In addition, the modular design of MorphIOâ€™s unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangi- ble character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.\n\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-3.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-5.png" /></a>\n  </div>\n</div>\n\n# Introduction\n\n**Programmable soft materials** have a great impact in many application fields, such as soft robots, material interfaces, and haptics.\n**However, programming of such materials is hard.**\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflowâ€”compiling code on a digital screen then trans- ferring it into the physical objectâ€”users need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the userâ€™s ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.\n\n\n# MorphIO\n\nThis paper introduces **MorphIO, entirely soft sensing and actuation modules** for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIOâ€™s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by recording and later playing back physical motions through tangible interaction. In addition, the modular design of MorphIOâ€™s unit allows the user to construct various shapes and topologies through magnetic connection, then synthesize multiple recorded motions to achieve more complex behaviors, such as bending, gripping, and walking.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/module.mp4" type="video/mp4"></source>\n</video>\n\n<br/>\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/bear.mp4" type="video/mp4"></source>\n</video>\n\n\n# System Overview\n\nThe programming workflow with MorphIO is the following:\n\n- **Step 1:** A user starts manipulating the MorphIO unit.\n\n- **Step 2:** The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.\n\n- **Step 3:** Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.\n\n- **Step 4:** By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.\n\nThe MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.\n\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-2.png" /></a>\n  </div>\n</div>\n\n\n# Entirely Soft Sensing and Actuation Modules\n\nOur main contribution is a design and fabrication method for **a conductive sponge sensor** that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to **sense the three-dimensional deformation by measuring the internal resistance value**; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.\n\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-2.png" /></a>\n  </div>\n</div>\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/mechanism.mp4" type="video/mp4"></source>\n</video>\n\n\n<br />\n\nMoreover, our **modular design** and **graphical interface** allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.\n\n\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-2.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui stackable one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-3.png" /></a>\n  </div>\n</div>\n\n\n\n\n# Fabrication Process\n\nThe fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/fabrication.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui stackable four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-4.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-6.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-7.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-7.png" /></a>\n  </div>\n</div>\n\n\n\n# Applications\n\nWe demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.\n\n<div class="figures ui stackable four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-4.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-3.png" /></a>\n  </div>\n</div>\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/locomotion.mp4" type="video/mp4"></source>\n</video>\n\n\n# Evaluation\n\nWe conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:\n\n- **RQ1:** Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?\n\n- **RQ2:** Does MorphIO increase the expressiveness of the physical motion?\n\nTo answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotionsâ€”happiness, anger, and sadnessâ€”of an animated character. We chose these emotions based on Ekmanâ€™s basic emotions for communication.\n\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-3.png" /></a>\n  </div>\n</div>\n\nThe average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: **RQ1: Yes, RQ2: No**.\n\nBased on our post interviews, we discuss the benefits and limitations of our approach: **1) tangible interactions are suitable for sculpting rough motion**, **2) programming allows for fine-tuning more precise adjustments**. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.\n\n\n# Future Vision\nWe believe this approachâ€™s potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motionâ€”not by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, **just like sculpting behaviors with clay**.\n\n\n<div class="figures ui stackable one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-14.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-14.png" /></a>\n  </div>\n</div>\n\n\n# Video Preview\n\n<div class="video-container">\n  <iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/[id]?autoplay=1&mute=1&rel=0&loop=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n</div>',bodyHtml:'<h1>MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction</h1>\n<p>Ryosuke Nakayama*, **Ryo Suzuki* **, Satoshi Nakamaru, Ryuma Niiyama, Yoshihiro Kawahara, Yasuaki Kakehi\n<br/>\n(the first two authors equally contributed)</p>\n<p><a href="https://dis2019.com/"><strong>The ACM conference on Designing Interactive Systems (DIS 2019)</strong></a>\n--- <strong>Best Paper Award</strong></p>\n<p>Links:\n[<strong><a href="http://ryosuzuki.org/publications/dis-2019-morphio.pdf">PDF</a></strong>]\n[<strong><a href="https://dl.acm.org/citation.cfm?id=3322337">ACM DL</a></strong>]\n[<strong><a href="https://www.youtube.com/watch?v=7nPlr3O9xu8">Video</a></strong>]</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/top.mp4" type="video/mp4"></source>\n</video>\n<h1>Abstract</h1>\n<p>We introduce MorphIO, entirely soft sensing and actuation modules for programming by demonstration of soft robots and shape-changing interfaces. MorphIOâ€™s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows both input and output of three-dimensional defor- mation of a soft material. Leveraging this capability, MorphIO enables a user to record and later playback physical motion of programmable shape-changing materials. In addition, the modular design of MorphIOâ€™s unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangi- ble character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.</p>\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-3.png" /></a>\n  </div>\n</div>\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-5.png" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p><strong>Programmable soft materials</strong> have a great impact in many application fields, such as soft robots, material interfaces, and haptics.\n<strong>However, programming of such materials is hard.</strong>\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflowâ€”compiling code on a digital screen then trans- ferring it into the physical objectâ€”users need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the userâ€™s ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.</p>\n<h1>MorphIO</h1>\n<p>This paper introduces <strong>MorphIO, entirely soft sensing and actuation modules</strong> for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIOâ€™s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by recording and later playing back physical motions through tangible interaction. In addition, the modular design of MorphIOâ€™s unit allows the user to construct various shapes and topologies through magnetic connection, then synthesize multiple recorded motions to achieve more complex behaviors, such as bending, gripping, and walking.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/module.mp4" type="video/mp4"></source>\n</video>\n<br/>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/bear.mp4" type="video/mp4"></source>\n</video>\n<h1>System Overview</h1>\n<p>The programming workflow with MorphIO is the following:</p>\n<ul>\n<li>\n<p><strong>Step 1:</strong> A user starts manipulating the MorphIO unit.</p>\n</li>\n<li>\n<p><strong>Step 2:</strong> The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.</p>\n</li>\n<li>\n<p><strong>Step 3:</strong> Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.</p>\n</li>\n<li>\n<p><strong>Step 4:</strong> By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.</p>\n</li>\n</ul>\n<p>The MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.</p>\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-2.png" /></a>\n  </div>\n</div>\n<h1>Entirely Soft Sensing and Actuation Modules</h1>\n<p>Our main contribution is a design and fabrication method for <strong>a conductive sponge sensor</strong> that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to <strong>sense the three-dimensional deformation by measuring the internal resistance value</strong>; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.</p>\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-2.png" /></a>\n  </div>\n</div>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/mechanism.mp4" type="video/mp4"></source>\n</video>\n<br />\n<p>Moreover, our <strong>modular design</strong> and <strong>graphical interface</strong> allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.</p>\n<div class="figures ui stackable two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-2.png" /></a>\n  </div>\n</div>\n<div class="figures ui stackable one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-3.png" /></a>\n  </div>\n</div>\n<h1>Fabrication Process</h1>\n<p>The fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/fabrication.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui stackable four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-4.png" /></a>\n  </div>\n</div>\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-6.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-7.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-7.png" /></a>\n  </div>\n</div>\n<h1>Applications</h1>\n<p>We demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.</p>\n<div class="figures ui stackable four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-4.png" /></a>\n  </div>\n</div>\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-3.png" /></a>\n  </div>\n</div>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/morphio/locomotion.mp4" type="video/mp4"></source>\n</video>\n<h1>Evaluation</h1>\n<p>We conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:</p>\n<ul>\n<li>\n<p><strong>RQ1:</strong> Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?</p>\n</li>\n<li>\n<p><strong>RQ2:</strong> Does MorphIO increase the expressiveness of the physical motion?</p>\n</li>\n</ul>\n<p>To answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotionsâ€”happiness, anger, and sadnessâ€”of an animated character. We chose these emotions based on Ekmanâ€™s basic emotions for communication.</p>\n<div class="figures ui stackable three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-3.png" /></a>\n  </div>\n</div>\n<p>The average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: <strong>RQ1: Yes, RQ2: No</strong>.</p>\n<p>Based on our post interviews, we discuss the benefits and limitations of our approach: <strong>1) tangible interactions are suitable for sculpting rough motion</strong>, <strong>2) programming allows for fine-tuning more precise adjustments</strong>. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.</p>\n<h1>Future Vision</h1>\n<p>We believe this approachâ€™s potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motionâ€”not by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, <strong>just like sculpting behaviors with clay</strong>.</p>\n<div class="figures ui stackable one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-14.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-14.png" /></a>\n  </div>\n</div>\n<h1>Video Preview</h1>\n<div class="video-container">\n  <iframe class="embed" width="100%" height="315" src="https://www.youtube.com/embed/[id]?autoplay=1&mute=1&rel=0&loop=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n</div>',dir:"content/output/projects",base:"morphio.json",ext:".json",sourceBase:"morphio.md",sourceExt:".md"}},ejaO:function(e){e.exports={id:"tabby",name:"Tabby",description:"Explorable Design for 3D Printing Textures",title:"Tabby: Explorable Design for 3D Printing Textures",authors:"Ryo Suzuki, Koji Yatani, Mark D. Gross, Tom Yeh",image:"tabby.png",conference:{name:"Pacific Graphics 2018",url:"http://sweb.cityu.edu.hk/pg2018/"},pdf:"pg-2018-tabby.pdf",video:"https://www.youtube.com/watch?v=rRgw8lH74CA","acm-dl":"https://diglib.eg.org/handle/10.2312/pg20181273",slide:"pg-2018-tabby-slide.pdf",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"tabby.json",ext:".json",sourceBase:"tabby.md",sourceExt:".md"}},jEBx:function(e){e.exports={id:"reactile",name:"Reactile",description:"Programming Swarm User Interfaces through Direct Physical Manipulation",title:"Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation",authors:"Ryo Suzuki, Jun Kato, Mark D. Gross, Tom Yeh",image:"reactile.jpg",conference:{name:"CHI 2018",url:"https://chi2018.acm.org/"},pdf:"chi-2018-reactile.pdf",video:"https://www.youtube.com/watch?v=Gb7brajKCVE","short-video":"https://www.youtube.com/watch?v=YT7vMJZjohU",slide:"chi-2018-reactile-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3173773",github:"https://github.com/ryosuzuki/reactile",abstract:"We explore a new approach to programming swarm user in- terfaces (Swarm UI) by leveraging direct physical manipu- lation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflowâ€”create elements, abstract attributes, specify behaviors, and propagate changesâ€” for Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studiesâ€”an in-class survey with 148 students and a lab interview with eight participantsâ€”confirm that our approach is intuitive and understandable for programming Swarm UIs.",bodyContent:"## Abstract\n\n![](/static/reactile.jpg)\n\nWe explore a new approach to programming swarm user in- terfaces (Swarm UI) by leveraging direct physical manipu- lation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflowâ€”create elements, abstract attributes, specify behaviors, and propagate changesâ€” for Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studiesâ€”an in-class survey with 148 students and a lab interview with eight participantsâ€”confirm that our approach is intuitive and understandable for programming Swarm UIs.",bodyHtml:'<h2>Abstract</h2>\n<p><img src="/static/reactile.jpg" alt=""></p>\n<p>We explore a new approach to programming swarm user in- terfaces (Swarm UI) by leveraging direct physical manipu- lation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflowâ€”create elements, abstract attributes, specify behaviors, and propagate changesâ€” for Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studiesâ€”an in-class survey with 148 students and a lab interview with eight participantsâ€”confirm that our approach is intuitive and understandable for programming Swarm UIs.</p>\n',dir:"content/output/projects",base:"reactile.json",ext:".json",sourceBase:"reactile.md",sourceExt:".md"}},mRot:function(e){e.exports={id:"refazer",name:"Refazer",description:"Learning Syntactic Program Transformations from Examples",title:"Learning Syntactic Program Transformations from Examples",authors:"Reudismam Rolim, Gustavo Soares, Loris Dâ€™Antoni, Oleksandr Polozov, Sumit Gulwani, Rohit Gheyi, Ryo Suzuki, and Bjoern Hartmann",image:"refazer.png",conference:{name:"ICSE 2017",url:"http://icse2017.gatech.edu/"},pdf:"icse-2017-refazer.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3097417",arxiv:"https://arxiv.org/abs/1608.09000",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"refazer.json",ext:".json",sourceBase:"refazer.md",sourceExt:".md"}},nWAr:function(e){e.exports={id:"atelier",name:"Atelier",description:"Repurposing Expert Crowdsourcing Tasks as Micro-internships",title:"Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships",authors:"Ryo Suzuki, Niloufar Salehi, Michelle S. Lam, Juan C. Marroquin, and Michael S. Bernstein",image:"atelier.jpg",conference:{name:"CHI 2016",url:"https://chi2016.acm.org/wp/"},pdf:"chi-2016-atelier.pdf",video:"https://www.youtube.com/watch?v=tBojZejtFQo",slide:"chi-2016-atelier-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=2858121",arxiv:"https://arxiv.org/abs/1602.06634",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"atelier.json",ext:".json",sourceBase:"atelier.md",sourceExt:".md"}},o0EK:function(e,t,a){var i={"./atelier.json":"nWAr","./dynablock.json":"GbvX","./flux-marker.json":"CTYI","./mixed-initiative.json":"PSd4","./morphio.json":"X0/d","./pep.json":"W/HP","./reactile.json":"jEBx","./refazer.json":"mRot","./tabby.json":"ejaO","./trace-diff.json":"Jg5j"};function n(e){var t=o(e);return a(t)}function o(e){var t=i[e];if(!(t+1)){var a=new Error("Cannot find module '"+e+"'");throw a.code="MODULE_NOT_FOUND",a}return t}n.keys=function(){return Object.keys(i)},n.resolve=o,e.exports=n,n.id="o0EK"},qg4i:function(e){e.exports=[{author:"Ryo Suzuki, Gustavo Soares, Elena Glassman, Andrew Head, Loris D'Antoni, and Bjoern Hartmann,",title:"Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments",pdf:"chi-2017-lbw.pdf",poster:"chi-2017-lbw-poster.pdf",booktitle:"Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems",series:"CHI EA '17",year:2017,isbn:"978-1-4503-4656-6",location:"Denver, Colorado, USA",pages:"2951--2958",numpages:8,url:"http://doi.acm.org/10.1145/3027063.3053187",doi:"10.1145/3027063.3053187",acmid:3053187,publisher:"ACM",address:"New York, NY, USA",keywords:"automated feedback, program synthesis, programming education"},{author:"Stanford Crowd Research Collective",title:"Daemo: A Self-Governed Crowdsourcing Marketplace",pdf:"uist-2015-daemo.pdf",booktitle:"Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology",series:"UIST '15 Adjunct",year:2015,isbn:"978-1-4503-3780-9",location:"Daegu, Kyungpook, Republic of Korea",pages:"101--102",numpages:2,url:"http://doi.acm.org/10.1145/2815585.2815739",doi:"10.1145/2815585.2815739",acmid:2815739,publisher:"ACM",address:"New York, NY, USA",keywords:"crowd research, crowd work., crowdsourcing"},{author:"Ryo Suzuki,",title:"Toward a Community Enhanced Programming Education",pdf:"chi-2015-workshop.pdf",slide:"chi-2015-workshop-slide.pdf",booktitle:"ACM CHI 2015 Symposium on Emerging Japanese HCI Research Collection",series:"CHI '15 Workshop",year:2015,location:"Seoul, Korea",publisher:"ACM",address:"New York, NY, USA"},{author:"Ryo Suzuki,",title:"Interactive and Collaborative Source Code Annotation",pdf:"icse-2015-cumiki.pdf",poster:"icse-2015-cumiki-poster.pdf",booktitle:"Proceedings of the 37th International Conference on Software Engineering - Volume 2",series:"ICSE '15 Poster",year:2015,location:"Florence, Italy",pages:"799--800",numpages:2,url:"http://dl.acm.org/citation.cfm?id=2819009.2819173",acmid:2819173,publisher:"IEEE Press",address:"Piscataway, NJ, USA"},{author:"Ryo Suzuki,",title:"Network Thresholds and Multiple Equilibria in the Diffusion of Content-Based Platforms",pdf:"wine-2014-network.pdf",poster:"wine-2014-network-poster.pdf",booktitle:"International Conference on Web and Internet Economics",series:"WINE '14 Poster",year:2014,location:"Beijing, China",publisher:"Springer",address:"New York, NY, USA"}]},vlRD:function(e,t,a){(window.__NEXT_P=window.__NEXT_P||[]).push(["/",function(){var e=a("RNiq");return{page:e.default||e}}])},yC4j:function(e){e.exports={bodyContent:"JST ACT-I (Mentor: Takeo Igarashi), 2018\n\nLeave a Nest Fellowship, 2018\n\nNakajima Foundation Scholarship, 2015\n\nJSPS Research Fellow DC1, 2013\n\nJASSO Fellow for Particularly Outstanding Students, 2013\n\nTohso Scholorship Fellowship, 2010\n\n[//]: # (JBMC Microsoft Award, 2013)\n\n[//]: # (Tech Crunch Tokyo 2013 Finalist, 2013)\n\n[//]: # (1st Prize Winner of University of Tokyo Entrepreneur Dojo, 2012)\n\n[//]: # (Honer of MOVIDA School founded by Taizo Son, 2012)",bodyHtml:"<p>JST ACT-I (Mentor: Takeo Igarashi), 2018</p>\n<p>Leave a Nest Fellowship, 2018</p>\n<p>Nakajima Foundation Scholarship, 2015</p>\n<p>JSPS Research Fellow DC1, 2013</p>\n<p>JASSO Fellow for Particularly Outstanding Students, 2013</p>\n<p>Tohso Scholorship Fellowship, 2010</p>\n",title:"JST ACT-I (Mentor: Takeo Igarashi), 2018",dir:"content/output",base:"fellowship.json",ext:".json",sourceBase:"fellowship.md",sourceExt:".md"}}},[["vlRD","5d41","9da1","ad9d"]]]);